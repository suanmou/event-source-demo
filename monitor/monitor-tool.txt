import os
import sys
import re
import time
import json
import threading
import http.server
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import glob
import zlib
from concurrent.futures import ThreadPoolExecutor
import mmap

# ======================== 常量配置 ========================
# 基础配置
STUNNEL_LOG_PATH = "/var/log/stunnel/stunnel.log"
STUNNEL_LOG_PATTERN = "/var/log/stunnel/stunnel.log.*"
PERSIST_BASE_DIR = "/var/lib/stunnel_monitor"
# 长连接配置
CONN_TIMEOUT = 604800  # 长连接超时（7天）
KEEP_ALIVE_LOG_ENABLE = True  # 是否开启保活检测
# 性能配置
LOG_BATCH_SIZE = 4096  # 日志批量读取缓冲区大小（字节）
ROTATE_LOG_THREADS = 2  # 轮转日志解析线程数
# 压缩/归档配置
COMPRESS_LEVEL = 6  # zlib压缩级别（1-9，6平衡性能和压缩率）
ARCHIVE_INTERVAL = "day"  # 归档粒度：hour/day
MAX_HISTORY_DAYS = 30  # 保留30天历史数据
MAX_HISTORY_COUNT = 100000  # 单归档文件最大记录数
# 其他配置
HTTP_PORT = 8000
HTTP_HOST = "0.0.0.0"
LOG_MONITOR_INTERVAL = 0.1
LOG_POS_INTERVAL = 5
PERSIST_INTERVAL = 30

# 目录初始化
os.makedirs(PERSIST_BASE_DIR, exist_ok=True)
os.makedirs(f"{PERSIST_BASE_DIR}/archive", exist_ok=True)
LOG_POS_FILE = f"{PERSIST_BASE_DIR}/log_pos.json"
PARSED_LOGS_FILE = f"{PERSIST_BASE_DIR}/parsed_logs.json"  # 已解析的轮转日志记录

# ======================== 日志解析正则（优化版） ========================
# 合并正则：减少匹配次数（按日志类型分组）
RE_STUNNEL = re.compile(
    r'^(?P<time>\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: '
    r'(?:Service \[(?P<service>\w+)\] '
    r'(?:accepted connection from (?P<src_ip>[\d\.]+):(?P<src_port>\d+)|'
    r'connected remote server from [\d\.]+:\d+ to (?P<dst_ip>[\d\.]+):(?P<dst_port>\d+)|'
    r'closed connection from (?P<close_src_ip>[\d\.]+):(?P<close_src_port>\d+)|'
    r'keepalive received from (?P<ka_src_ip>[\d\.]+):(?P<ka_src_port>\d+)))$'  # 保活日志
)

# ======================== 工具函数 ========================
def get_archive_file_path(timestamp: float) -> str:
    """根据时间戳生成归档文件路径"""
    dt = datetime.fromtimestamp(timestamp)
    if ARCHIVE_INTERVAL == "hour":
        filename = dt.strftime("%Y%m%d_%H.json.zlib")
    else:
        filename = dt.strftime("%Y%m%d.json.zlib")
    return f"{PERSIST_BASE_DIR}/archive/{filename}"

def compress_data(data: dict) -> bytes:
    """压缩JSON数据"""
    json_str = json.dumps(data, ensure_ascii=False)
    return zlib.compress(json_str.encode("utf-8"), COMPRESS_LEVEL)

def decompress_data(data_bytes: bytes) -> dict:
    """解压数据"""
    json_str = zlib.decompress(data_bytes).decode("utf-8")
    return json.loads(json_str)

def clean_expired_archives():
    """清理过期归档文件"""
    cutoff = time.time() - MAX_HISTORY_DAYS * 86400
    archive_files = glob.glob(f"{PERSIST_BASE_DIR}/archive/*.json.zlib")
    for file in archive_files:
        if os.path.getmtime(file) < cutoff:
            os.remove(file)
            logging.info(f"清理过期归档文件: {file}")

# ======================== 连接状态模型（长连接+压缩优化） ========================
class ConnectionState:
    def __init__(self):
        self.lock = threading.RLock()  # 递归锁，支持嵌套加锁
        self.online_conns: Dict[str, dict] = {}  # 在线连接
        self.conn_id_set = set()  # 去重集合
        self.timeout = CONN_TIMEOUT
        self.parsed_logs = self._load_parsed_logs()  # 已解析的轮转日志

        # 加载在线连接（从最新归档/持久化文件）
        self._load_online_conns()
        # 清理过期归档
        clean_expired_archives()

    def _parse_log_time(self, time_str: str) -> float:
        try:
            dt = datetime.strptime(time_str, "%Y.%m.%d %H:%M:%S")
            return dt.timestamp()
        except Exception as e:
            logging.error(f"解析时间失败: {time_str}, 错误: {e}")
            return 0.0

    def _gen_conn_id(self, service: str, src_ip: str, src_port: str, create_time: float) -> str:
        """优化：加入微秒级时间戳，避免长连接源端口复用冲突"""
        return f"{service}_{src_ip}_{src_port}_{int(create_time * 1000)}"

    def _load_parsed_logs(self) -> set:
        """加载已解析的轮转日志列表"""
        if not os.path.exists(PARSED_LOGS_FILE):
            return set()
        try:
            with open(PARSED_LOGS_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception as e:
            logging.error(f"加载已解析日志列表失败: {e}")
            return set()

    def _save_parsed_logs(self):
        """保存已解析的轮转日志列表"""
        try:
            with open(PARSED_LOGS_FILE, "w", encoding="utf-8") as f:
                json.dump(list(self.parsed_logs), f)
        except Exception as e:
            logging.error(f"保存已解析日志列表失败: {e}")

    def _load_online_conns(self):
        """加载在线连接（从最新归档）"""
        # 找到最新的归档文件
        archive_files = glob.glob(f"{PERSIST_BASE_DIR}/archive/*.json.zlib")
        if not archive_files:
            logging.info("无归档文件，在线连接为空")
            return
        latest_archive = max(archive_files, key=os.path.getmtime())
        try:
            with open(latest_archive, "rb") as f:
                data = decompress_data(f.read())
                online = data.get("online", {})
                now = time.time()
                # 过滤未超时的长连接
                for conn_id, conn in online.items():
                    if now - conn["create_time"] < self.timeout:
                        self.online_conns[conn_id] = conn
                        self.conn_id_set.add(conn_id)
            logging.info(f"从归档{latest_archive}加载在线连接{len(self.online_conns)}条")
        except Exception as e:
            logging.error(f"加载归档文件失败: {e}")

    def _save_history(self, history_conns: List[dict]):
        """保存历史记录到归档文件（压缩+分片）"""
        if not history_conns:
            return
        try:
            # 按归档粒度分组
            archive_groups = {}
            for conn in history_conns:
                ts = conn.get("create_time", time.time())
                path = get_archive_file_path(ts)
                if path not in archive_groups:
                    archive_groups[path] = []
                archive_groups[path].append(conn)

            # 写入归档（追加模式）
            for path, conns in archive_groups.items():
                # 读取已有数据
                existing = []
                if os.path.exists(path):
                    with open(path, "rb") as f:
                        existing = decompress_data(f.read()).get("history", [])
                # 合并并限制数量
                combined = existing + conns
                if len(combined) > MAX_HISTORY_COUNT:
                    combined = combined[-MAX_HISTORY_COUNT:]
                # 压缩保存
                data = {"history": combined, "update_time": time.time()}
                with open(path, "wb") as f:
                    f.write(compress_data(data))
            logging.debug(f"保存{len(history_conns)}条历史记录到归档")
        except Exception as e:
            logging.error(f"保存历史记录失败: {e}")

    def _save_online_conns(self):
        """保存在线连接（压缩）"""
        try:
            path = get_archive_file_path(time.time())
            # 读取已有数据
            existing_history = []
            if os.path.exists(path):
                with open(path, "rb") as f:
                    existing_history = decompress_data(f.read()).get("history", [])
            # 保存在线连接+历史
            data = {
                "online": self.online_conns,
                "history": existing_history,
                "update_time": time.time()
            }
            with open(path, "wb") as f:
                f.write(compress_data(data))
            logging.debug("保存在线连接到归档")
        except Exception as e:
            logging.error(f"保存在线连接失败: {e}")

    def update_conn_from_log(self, log_line: str):
        """优化：单函数处理所有日志类型，减少正则匹配次数"""
        match = RE_STUNNEL.match(log_line.strip())
        if not match:
            return
        groups = match.groupdict()
        time_str = groups.get("time")
        if not time_str:
            return
        ts = self._parse_log_time(time_str)
        if ts == 0:
            return

        with self.lock:
            # 处理连接建立
            if groups.get("src_ip") and groups.get("src_port") and groups.get("service"):
                conn_id = self._gen_conn_id(groups["service"], groups["src_ip"], groups["src_port"], ts)
                if conn_id not in self.conn_id_set:
                    self.online_conns[conn_id] = {
                        "conn_id": conn_id,
                        "service": groups["service"],
                        "src_ip": groups["src_ip"],
                        "src_port": groups["src_port"],
                        "create_time": ts,
                        "create_time_str": time_str,
                        "dst_ip": None,
                        "dst_port": None,
                        "close_time": None,
                        "duration": None,
                        "last_ka_time": ts,  # 最后保活时间（长连接）
                        "status": "online"
                    }
                    self.conn_id_set.add(conn_id)
                    logging.debug(f"新增长连接: {conn_id}")
            # 处理下游转发
            elif groups.get("dst_ip") and groups.get("dst_port") and groups.get("service"):
                for conn in self.online_conns.values():
                    if conn["service"] == groups["service"] and conn["dst_ip"] is None:
                        conn["dst_ip"] = groups["dst_ip"]
                        conn["dst_port"] = groups["dst_port"]
                        break
            # 处理连接关闭
            elif groups.get("close_src_ip") and groups.get("close_src_port") and groups.get("service"):
                target_conn_id = None
                for conn_id, conn in self.online_conns.items():
                    if (conn["service"] == groups["service"] and 
                        conn["src_ip"] == groups["close_src_ip"] and 
                        conn["src_port"] == groups["close_src_port"]):
                        target_conn_id = conn_id
                        break
                if target_conn_id:
                    conn = self.online_conns[target_conn_id]
                    conn["close_time"] = ts
                    conn["duration"] = ts - conn["create_time"]
                    conn["status"] = "closed"
                    # 移入历史
                    self._save_history([conn])
                    del self.online_conns[target_conn_id]
                    logging.debug(f"关闭长连接: {target_conn_id}, 时长: {conn['duration']:.2f}秒")
            # 处理保活（长连接）
            elif KEEP_ALIVE_LOG_ENABLE and groups.get("ka_src_ip") and groups.get("ka_src_port") and groups.get("service"):
                for conn_id, conn in self.online_conns.items():
                    if (conn["service"] == groups["service"] and 
                        conn["src_ip"] == groups["ka_src_ip"] and 
                        conn["src_port"] == groups["ka_src_port"]):
                        conn["last_ka_time"] = ts  # 更新最后保活时间
                        break

    def clean_timeout_conns(self):
        """优化：基于最后保活时间清理长连接"""
        now = time.time()
        timeout_conns = []
        with self.lock:
            for conn_id, conn in list(self.online_conns.items()):
                # 长连接超时判断：当前时间 - 最后保活时间 > 超时时间
                if now - conn["last_ka_time"] >= self.timeout:
                    conn["close_time"] = now
                    conn["duration"] = now - conn["create_time"]
                    conn["status"] = "timeout"
                    timeout_conns.append(conn)
                    del self.online_conns[conn_id]
                    logging.debug(f"清理超时长连接: {conn_id}")
        # 批量保存超时连接到历史
        if timeout_conns:
            self._save_history(timeout_conns)

    def get_online_conns(self) -> List[dict]:
        """获取在线长连接（带实时时长+最后保活时间）"""
        now = time.time()
        with self.lock:
            online = []
            for conn in self.online_conns.values():
                conn_copy = conn.copy()
                conn_copy["duration"] = round(now - conn_copy["create_time"], 2)
                conn_copy["last_ka_time_str"] = datetime.fromtimestamp(conn_copy["last_ka_time"]).strftime("%Y.%m.%d %H:%M:%S")
                online.append(conn_copy)
        return online

    def get_history_conns(self, start_time: float = None, end_time: float = None, limit: int = 1000) -> List[dict]:
        """按时间范围查询历史记录（优化：从归档加载）"""
        history = []
        # 遍历归档文件
        archive_files = glob.glob(f"{PERSIST_BASE_DIR}/archive/*.json.zlib")
        archive_files.sort(key=os.path.getmtime, reverse=True)  # 从新到旧

        with self.lock:
            for file in archive_files:
                try:
                    with open(file, "rb") as f:
                        data = decompress_data(f.read())
                        for conn in data.get("history", []):
                            ts = conn.get("create_time", 0)
                            # 时间范围过滤
                            if start_time and ts < start_time:
                                continue
                            if end_time and ts > end_time:
                                continue
                            history.append(conn)
                            if len(history) >= limit:
                                break
                    if len(history) >= limit:
                        break
                except Exception as e:
                    logging.error(f"读取归档{file}失败: {e}")
            return history[:limit]

    def get_stats(self) -> dict:
        """获取统计数据（优化：包含长连接占比）"""
        now = time.time()
        with self.lock:
            online_count = len(self.online_conns)
            # 统计历史记录总数
            history_count = 0
            archive_files = glob.glob(f"{PERSIST_BASE_DIR}/archive/*.json.zlib")
            for file in archive_files:
                try:
                    with open(file, "rb") as f:
                        data = decompress_data(f.read())
                        history_count += len(data.get("history", []))
                except Exception as e:
                    logging.error(f"统计归档{file}失败: {e}")
            # 长连接占比（在线连接中持续>24小时的）
            long_conn_count = 0
            for conn in self.online_conns.values():
                if now - conn["create_time"] > 86400:
                    long_conn_count += 1
            long_conn_ratio = round(long_conn_count / online_count, 2) if online_count > 0 else 0.0
            # 按下游统计
            dst_stats = {}
            for conn in self.online_conns.values():
                dst = f"{conn['dst_ip']}:{conn['dst_port']}" if conn["dst_ip"] else "unknown"
                dst_stats[dst] = dst_stats.get(dst, 0) + 1
            # 历史下游统计（抽样）
            sample_history = self.get_history_conns(limit=1000)
            for conn in sample_history:
                dst = f"{conn['dst_ip']}:{conn['dst_port']}" if conn["dst_ip"] else "unknown"
                dst_stats[dst] = dst_stats.get(dst, 0) + 1

        return {
            "online_count": online_count,
            "long_conn_count": long_conn_count,
            "long_conn_ratio": long_conn_ratio,
            "history_count": history_count,
            "dst_stats": dst_stats,
            "update_time": now,
            "update_time_str": datetime.fromtimestamp(now).strftime("%Y.%m.%d %H:%M:%S")
        }

    def save_persist_data(self):
        """统一持久化入口"""
        self._save_online_conns()
        self._save_parsed_logs()
        clean_expired_archives()

# ======================== 日志监控类（性能优化版） ========================
class StunnelLogMonitor(threading.Thread):
    def __init__(self, log_path: str, conn_state: ConnectionState):
        super().__init__(daemon=True)
        self.log_path = log_path
        self.log_pattern = STUNNEL_LOG_PATTERN
        self.conn_state = conn_state
        self.running = True
        self.file_handle = None
        self.file_inode = 0
        self.file_pos = 0
        self.log_pos_file = LOG_POS_FILE
        self.executor = ThreadPoolExecutor(max_workers=ROTATE_LOG_THREADS)

        # 初始化
        self._load_log_pos()
        self._backtrack_rotated_logs()  # 并行回溯轮转日志
        self._init_file()

    def _load_log_pos(self):
        """加载日志读取位置"""
        if not os.path.exists(self.log_pos_file):
            logging.info("无日志位置记录，从新开始")
            return
        try:
            with open(self.log_pos_file, "r", encoding="utf-8") as f:
                pos_data = json.load(f)
                self.file_inode = pos_data.get("inode", 0)
                self.file_pos = pos_data.get("pos", 0)
            logging.info(f"恢复日志读取位置: inode={self.file_inode}, pos={self.file_pos}")
        except Exception as e:
            logging.error(f"加载日志位置失败: {e}")

    def _save_log_pos(self):
        """保存日志读取位置"""
        try:
            if not self.file_handle:
                return
            pos_data = {
                "inode": self.file_inode,
                "pos": self.file_pos,
                "path": self.log_path,
                "update_time": time.time()
            }
            with open(self.log_pos_file, "w", encoding="utf-8") as f:
                json.dump(pos_data, f)
        except Exception as e:
            logging.error(f"保存日志位置失败: {e}")

    def _parse_log_file_mmap(self, file_path: str):
        """用mmap解析大日志文件（高性能）"""
        if file_path in self.conn_state.parsed_logs:
            logging.info(f"跳过已解析的日志: {file_path}")
            return
        logging.info(f"开始解析大日志文件: {file_path}")
        try:
            with open(file_path, "rb") as f:
                # mmap加载整个文件
                mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
                # 按行读取（批量）
                lines = mm.read().decode("utf-8", errors="ignore").splitlines()
                mm.close()
                # 批量解析
                for line in lines:
                    self.conn_state.update_conn_from_log(line)
            # 标记为已解析
            self.conn_state.parsed_logs.add(file_path)
            logging.info(f"解析完成: {file_path}，共{len(lines)}行")
        except Exception as e:
            logging.error(f"解析日志{file_path}失败: {e}")

    def _backtrack_rotated_logs(self):
        """并行回溯轮转日志"""
        rotated_logs = glob.glob(self.log_pattern)
        rotated_logs = [f for f in rotated_logs if f != self.log_path and f not in self.conn_state.parsed_logs]
        rotated_logs.sort(key=lambda x: os.path.getmtime(x))  # 旧→新

        if not rotated_logs:
            logging.info("无未解析的轮转日志")
            return
        logging.info(f"发现{len(rotated_logs)}个未解析轮转日志，并行解析")

        # 提交到线程池
        futures = [self.executor.submit(self._parse_log_file_mmap, log_file) for log_file in rotated_logs]
        # 等待所有任务完成
        for future in futures:
            try:
                future.result()
            except Exception as e:
                logging.error(f"解析轮转日志失败: {e}")
        # 保存已解析列表
        self.conn_state._save_parsed_logs()

    def _init_file(self):
        """初始化主日志文件"""
        try:
            if self.file_handle:
                self.file_handle.close()
            self.file_handle = open(self.log_path, "rb")  # 二进制模式便于批量读取
            stat = os.stat(self.log_path)
            if stat.st_ino == self.file_inode:
                self.file_handle.seek(self.file_pos)
            else:
                self.file_pos = 0
                self.file_handle.seek(0)
            self.file_inode = stat.st_ino
        except Exception as e:
            logging.error(f"初始化主日志失败: {e}")
            self.file_handle = None

    def _check_log_rotate(self):
        """检查日志轮转"""
        try:
            stat = os.stat(self.log_path)
            if stat.st_ino != self.file_inode:
                logging.info(f"检测到日志轮转: 旧inode={self.file_inode}, 新inode={stat.st_ino}")
                # 解析剩余内容（批量）
                self.file_handle.seek(self.file_pos)
                remaining = self.file_handle.read().decode("utf-8", errors="ignore")
                if remaining:
                    for line in remaining.splitlines():
                        self.conn_state.update_conn_from_log(line)
                # 重新初始化
                self._init_file()
            elif stat.st_size < self.file_pos:
                self.file_pos = 0
                self.file_handle.seek(0)
                logging.info("主日志被截断，重置读取位置")
        except Exception as e:
            logging.error(f"检查日志轮转失败: {e}")
            self._init_file()

    def run(self):
        """日志监控主逻辑（批量读取）"""
        pos_save_timer = time.time()
        while self.running:
            try:
                if not self.file_handle:
                    time.sleep(1)
                    self._init_file()
                    continue

                self._check_log_rotate()

                # 批量读取日志（高性能）
                self.file_handle.seek(self.file_pos)
                batch = self.file_handle.read(LOG_BATCH_SIZE).decode("utf-8", errors="ignore")
                if batch:
                    lines = batch.splitlines()
                    # 批量解析
                    for line in lines:
                        self.conn_state.update_conn_from_log(line)
                    # 更新读取位置（注意：最后一行可能不完整，需处理）
                    if batch[-1] != "\n":
                        # 最后一行不完整，回退到行首
                        line_count = len(lines) - 1
                        self.file_pos -= len(lines[-1]) if line_count > 0 else len(batch)
                    else:
                        self.file_pos = self.file_handle.tell()

                # 定期保存位置
                if time.time() - pos_save_timer >= LOG_POS_INTERVAL:
                    self._save_log_pos()
                    pos_save_timer = time.time()

                time.sleep(LOG_MONITOR_INTERVAL)
            except Exception as e:
                logging.error(f"日志监控异常: {e}")
                time.sleep(1)

    def stop(self):
        self.running = False
        if self.file_handle:
            self.file_handle.close()
        self.executor.shutdown(wait=True)
        self._save_log_pos()

# ======================== HTTP接口（适配长连接+时间范围查询） ========================
class ConnMonitorHTTPHandler(http.server.BaseHTTPRequestHandler):
    conn_state: ConnectionState = None

    def _send_json_response(self, data: dict, status_code: int = 200):
        self.send_response(status_code)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8"))

    def do_GET(self):
        try:
            # 解析查询参数
            params = {}
            if "?" in self.path:
                path, query = self.path.split("?", 1)
                params = dict(q.split("=") for q in query.split("&") if "=" in q)
            else:
                path = self.path

            if path == "/api/online":
                online = self.conn_state.get_online_conns()
                self._send_json_response({"code":0, "msg":"success", "data":online})
            elif path == "/api/history":
                # 时间范围参数（时间戳）
                start_time = float(params.get("start", 0)) if params.get("start") else None
                end_time = float(params.get("end", 0)) if params.get("end") else None
                limit = int(params.get("limit", 1000))
                history = self.conn_state.get_history_conns(start_time, end_time, limit)
                self._send_json_response({"code":0, "msg":"success", "data":history})
            elif path == "/api/stats":
                stats = self.conn_state.get_stats()
                self._send_json_response({"code":0, "msg":"success", "data":stats})
            elif path == "/api/health":
                self._send_json_response({"code":0, "msg":"healthy", "data":{"timestamp":time.time()}})
            else:
                self._send_json_response({"code":404, "msg":"not found"}, 404)
        except Exception as e:
            logging.error(f"HTTP接口异常: {e}")
            self._send_json_response({"code":500, "msg":str(e)}, 500)

# ======================== 持久化线程 ========================
class PersistThread(threading.Thread):
    def __init__(self, conn_state: ConnectionState, interval: int = PERSIST_INTERVAL):
        super().__init__(daemon=True)
        self.conn_state = conn_state
        self.interval = interval
        self.running = True

    def run(self):
        while self.running:
            time.sleep(self.interval)
            self.conn_state.save_persist_data()
            self.conn_state.clean_timeout_conns()

    def stop(self):
        self.running = False

# ======================== 主程序 ========================
def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f"{PERSIST_BASE_DIR}/monitor.log", encoding="utf-8")
        ]
    )

    conn_state = ConnectionState()
    ConnMonitorHTTPHandler.conn_state = conn_state

    # 启动日志监控
    log_monitor = StunnelLogMonitor(STUNNEL_LOG_PATH, conn_state)
    log_monitor.start()
    logging.info("日志监控线程启动（高性能模式）")

    # 启动持久化线程
    persist_thread = PersistThread(conn_state)
    persist_thread.start()
    logging.info("持久化线程启动")

    # 启动HTTP服务
    server_address = (HTTP_HOST, HTTP_PORT)
    httpd = http.server.ThreadingHTTPServer(server_address, ConnMonitorHTTPHandler)
    logging.info(f"HTTP服务启动: http://{HTTP_HOST}:{HTTP_PORT}")

    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        logging.info("接收到停止信号，正在退出...")
        log_monitor.stop()
        persist_thread.stop()
        httpd.shutdown()
        conn_state.save_persist_data()
        logging.info("程序正常退出")

if __name__ == "__main__":
    main()





import os
import sys
import re
import time
import json
import threading
import http.server
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import glob

# ======================== 常量配置 ========================
STUNNEL_LOG_PATH = "/var/log/stunnel/stunnel.log"  # Stunnel主日志路径
STUNNEL_LOG_PATTERN = "/var/log/stunnel/stunnel.log.*"  # 轮转日志匹配模式
PERSIST_FILE = "/var/lib/stunnel_monitor/data.json"  # 连接状态持久化
LOG_POS_FILE = "/var/lib/stunnel_monitor/log_pos.json"  # 日志读取位置持久化
CONN_TIMEOUT = 86400  # 连接超时时间（秒）
PERSIST_INTERVAL = 30  # 状态持久化间隔
LOG_POS_INTERVAL = 5  # 日志位置持久化间隔（秒）
HTTP_PORT = 8000
HTTP_HOST = "0.0.0.0"
LOG_MONITOR_INTERVAL = 0.1

# ======================== 日志解析正则 ========================
RE_ACCEPT = re.compile(
    r'^(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: Service \[(\w+)\] accepted connection from ([\d\.]+):(\d+)$'
)
RE_CONNECT_REMOTE = re.compile(
    r'^(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: Service \[(\w+)\] connected remote server from [\d\.]+:\d+ to ([\d\.]+):(\d+)$'
)
RE_CLOSE = re.compile(
    r'^(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: Service \[(\w+)\] closed connection from ([\d\.]+):(\d+)$'
)

# ======================== 连接状态模型（增强重启恢复） ========================
class ConnectionState:
    def __init__(self):
        self.lock = threading.Lock()
        self.online_conns: Dict[str, dict] = {}  # 在线连接
        self.history_conns: List[dict] = []  # 历史连接
        self.conn_id_set = set()  # 连接ID去重集合（避免重复解析）
        self.timeout = CONN_TIMEOUT

        # 初始化：先加载连接状态，再加载日志位置
        self._load_persist_data()

    def _parse_log_time(self, time_str: str) -> float:
        """解析日志时间为时间戳"""
        try:
            dt = datetime.strptime(time_str, "%Y.%m.%d %H:%M:%S")
            return dt.timestamp()
        except Exception as e:
            logging.error(f"解析时间失败: {time_str}, 错误: {e}")
            return 0.0

    def _gen_conn_id(self, service: str, src_ip: str, src_port: str, create_time: float) -> str:
        """生成唯一连接ID"""
        return f"{service}_{src_ip}_{src_port}_{int(create_time)}"

    def _load_persist_data(self):
        """加载连接状态持久化数据"""
        if not os.path.exists(PERSIST_FILE):
            os.makedirs(os.path.dirname(PERSIST_FILE), exist_ok=True)
            return
        try:
            with open(PERSIST_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # 加载历史记录并去重
                history = data.get("history", [])
                for conn in history:
                    conn_id = conn.get("conn_id")
                    if conn_id and conn_id not in self.conn_id_set:
                        self.history_conns.append(conn)
                        self.conn_id_set.add(conn_id)
                # 加载在线连接（过滤超时）
                online = data.get("online", {})
                now = time.time()
                for conn_id, conn in online.items():
                    if conn_id not in self.conn_id_set and now - conn["create_time"] < self.timeout:
                        self.online_conns[conn_id] = conn
                        self.conn_id_set.add(conn_id)
                    elif conn_id not in self.conn_id_set:
                        # 超时连接移入历史
                        conn["close_time"] = now
                        conn["duration"] = now - conn["create_time"]
                        conn["status"] = "timeout"
                        self.history_conns.append(conn)
                        self.conn_id_set.add(conn_id)
            logging.info(f"加载持久化数据：历史{len(self.history_conns)}条，在线{len(self.online_conns)}条")
        except Exception as e:
            logging.error(f"加载连接状态失败: {e}")

    def _save_persist_data(self):
        """保存连接状态"""
        try:
            with self.lock:
                data = {
                    "online": self.online_conns,
                    "history": self.history_conns[-10000:],  # 仅保留最近1万条
                }
            with open(PERSIST_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
            logging.debug("连接状态持久化成功")
        except Exception as e:
            logging.error(f"保存连接状态失败: {e}")

    def update_accept_conn(self, log_line: str):
        """处理连接建立日志（幂等）"""
        match = RE_ACCEPT.match(log_line.strip())
        if not match:
            return
        time_str, service, src_ip, src_port = match.groups()
        create_time = self._parse_log_time(time_str)
        if create_time == 0:
            return
        conn_id = self._gen_conn_id(service, src_ip, src_port, create_time)
        if conn_id in self.conn_id_set:
            return  # 已解析过，跳过
        with self.lock:
            self.online_conns[conn_id] = {
                "conn_id": conn_id,
                "service": service,
                "src_ip": src_ip,
                "src_port": src_port,
                "create_time": create_time,
                "create_time_str": time_str,
                "dst_ip": None,
                "dst_port": None,
                "close_time": None,
                "duration": None,
                "status": "online"
            }
            self.conn_id_set.add(conn_id)
        logging.debug(f"新增连接: {conn_id}")

    def update_remote_conn(self, log_line: str):
        """处理下游转发日志"""
        match = RE_CONNECT_REMOTE.match(log_line.strip())
        if not match:
            return
        time_str, service, dst_ip, dst_port = match.groups()
        with self.lock:
            for conn in self.online_conns.values():
                if conn["service"] == service and conn["dst_ip"] is None:
                    conn["dst_ip"] = dst_ip
                    conn["dst_port"] = dst_port
                    break
        logging.debug(f"更新下游: {service} -> {dst_ip}:{dst_port}")

    def update_close_conn(self, log_line: str):
        """处理连接关闭日志（幂等）"""
        match = RE_CLOSE.match(log_line.strip())
        if not match:
            return
        time_str, service, src_ip, src_port = match.groups()
        close_time = self._parse_log_time(time_str)
        if close_time == 0:
            return
        with self.lock:
            # 查找匹配的在线连接
            target_conn_id = None
            for conn_id, conn in self.online_conns.items():
                if conn["service"] == service and conn["src_ip"] == src_ip and conn["src_port"] == src_port:
                    target_conn_id = conn_id
                    break
            if target_conn_id and target_conn_id not in self.conn_id_set:
                return
            if target_conn_id:
                # 更新状态并移入历史
                conn = self.online_conns[target_conn_id]
                conn["close_time"] = close_time
                conn["duration"] = close_time - conn["create_time"]
                conn["status"] = "closed"
                self.history_conns.append(conn)
                del self.online_conns[target_conn_id]
                logging.debug(f"关闭连接: {target_conn_id}, 时长: {conn['duration']:.2f}秒")

    def clean_timeout_conns(self):
        """清理超时连接"""
        now = time.time()
        with self.lock:
            for conn_id, conn in list(self.online_conns.items()):
                if now - conn["create_time"] >= self.timeout:
                    conn["close_time"] = now
                    conn["duration"] = now - conn["create_time"]
                    conn["status"] = "timeout"
                    self.history_conns.append(conn)
                    del self.online_conns[conn_id]
                    logging.debug(f"清理超时连接: {conn_id}")

    def get_online_conns(self) -> List[dict]:
        """获取在线连接（带实时时长）"""
        now = time.time()
        with self.lock:
            online = []
            for conn in self.online_conns.values():
                conn_copy = conn.copy()
                conn_copy["duration"] = now - conn_copy["create_time"]
                online.append(conn_copy)
        return online

    def get_history_conns(self, limit: int = 1000) -> List[dict]:
        """获取历史连接"""
        with self.lock:
            return self.history_conns[-limit:]

    def get_stats(self) -> dict:
        """获取统计数据"""
        now = time.time()
        with self.lock:
            online_count = len(self.online_conns)
            history_count = len(self.history_conns)
            durations = [c["duration"] for c in self.history_conns if c.get("duration")]
            avg_duration = sum(durations)/len(durations) if durations else 0.0
            # 按下游统计
            dst_stats = {}
            for conn in self.online_conns.values():
                dst = f"{conn['dst_ip']}:{conn['dst_port']}" if conn["dst_ip"] else "unknown"
                dst_stats[dst] = dst_stats.get(dst, 0) + 1
            for conn in self.history_conns:
                dst = f"{conn['dst_ip']}:{conn['dst_port']}" if conn["dst_ip"] else "unknown"
                dst_stats[dst] = dst_stats.get(dst, 0) + 1
        return {
            "online_count": online_count,
            "history_count": history_count,
            "avg_duration": round(avg_duration, 2),
            "dst_stats": dst_stats,
            "update_time": now,
            "update_time_str": datetime.fromtimestamp(now).strftime("%Y.%m.%d %H:%M:%S")
        }

# ======================== 日志监控类（增强重启回溯） ========================
class StunnelLogMonitor(threading.Thread):
    def __init__(self, log_path: str, conn_state: ConnectionState):
        super().__init__(daemon=True)
        self.log_path = log_path
        self.log_pattern = STUNNEL_LOG_PATTERN
        self.conn_state = conn_state
        self.running = True
        self.file_handle = None
        self.file_inode = 0
        self.file_pos = 0
        self.log_pos_file = LOG_POS_FILE

        # 初始化：先回溯旧日志，再恢复读取位置
        self._init_log_backtrack()
        self._load_log_pos()
        self._init_file()

    def _load_log_pos(self):
        """加载上次的日志读取位置"""
        if not os.path.exists(self.log_pos_file):
            logging.info("无日志位置记录，从新开始")
            return
        try:
            with open(self.log_pos_file, "r", encoding="utf-8") as f:
                pos_data = json.load(f)
                self.file_inode = pos_data.get("inode", 0)
                self.file_pos = pos_data.get("pos", 0)
            logging.info(f"恢复日志读取位置: inode={self.file_inode}, pos={self.file_pos}")
        except Exception as e:
            logging.error(f"加载日志位置失败: {e}")

    def _save_log_pos(self):
        """保存当前日志读取位置"""
        try:
            if not self.file_handle:
                return
            pos_data = {
                "inode": self.file_inode,
                "pos": self.file_pos,
                "path": self.log_path,
                "update_time": time.time()
            }
            with open(self.log_pos_file, "w", encoding="utf-8") as f:
                json.dump(pos_data, f)
            logging.debug("日志位置保存成功")
        except Exception as e:
            logging.error(f"保存日志位置失败: {e}")

    def _get_rotated_logs(self):
        """获取所有轮转的旧日志（按修改时间升序）"""
        rotated_logs = glob.glob(self.log_pattern)
        # 过滤主日志
        rotated_logs = [f for f in rotated_logs if f != self.log_path]
        # 按修改时间排序（旧→新）
        rotated_logs.sort(key=lambda x: os.path.getmtime(x))
        return rotated_logs

    def _parse_log_file(self, file_path: str):
        """解析单个日志文件（全量）"""
        logging.info(f"开始解析日志文件: {file_path}")
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    # 解析日志行
                    self.conn_state.update_accept_conn(line)
                    self.conn_state.update_remote_conn(line)
                    self.conn_state.update_close_conn(line)
            logging.info(f"解析完成: {file_path}，共处理{sum(1 for _ in open(file_path, 'r'))}行")
        except Exception as e:
            logging.error(f"解析日志文件{file_path}失败: {e}")

    def _init_log_backtrack(self):
        """启动时回溯所有未处理的轮转日志"""
        rotated_logs = self._get_rotated_logs()
        if not rotated_logs:
            logging.info("无轮转日志需要回溯")
            return
        logging.info(f"发现{len(rotated_logs)}个轮转日志，开始回溯")
        for log_file in rotated_logs:
            self._parse_log_file(log_file)
        logging.info("轮转日志回溯完成")

    def _init_file(self):
        """初始化主日志文件句柄"""
        try:
            if self.file_handle:
                self.file_handle.close()
            self.file_handle = open(self.log_path, "r", encoding="utf-8")
            stat = os.stat(self.log_path)
            # 如果inode匹配，恢复上次位置；否则从0开始
            if stat.st_ino == self.file_inode:
                self.file_handle.seek(self.file_pos)
                logging.info(f"恢复主日志读取位置: pos={self.file_pos}")
            else:
                self.file_pos = 0
                self.file_handle.seek(0)
                logging.info(f"主日志inode变化，重置读取位置到0")
            self.file_inode = stat.st_ino
        except Exception as e:
            logging.error(f"初始化主日志失败: {e}")
            self.file_handle = None

    def _check_log_rotate(self):
        """检查日志轮转"""
        try:
            stat = os.stat(self.log_path)
            # inode变化 → 日志轮转
            if stat.st_ino != self.file_inode:
                logging.info(f"检测到日志轮转: 旧inode={self.file_inode}, 新inode={stat.st_ino}")
                # 先解析旧日志剩余内容
                self.file_handle.seek(self.file_pos)
                remaining = self.file_handle.read()
                if remaining:
                    for line in remaining.splitlines():
                        self.conn_state.update_accept_conn(line)
                        self.conn_state.update_remote_conn(line)
                        self.conn_state.update_close_conn(line)
                # 重新初始化文件
                self._init_file()
            # 文件被截断（如copytruncate）
            elif stat.st_size < self.file_pos:
                self.file_pos = 0
                self.file_handle.seek(0)
                logging.info("主日志被截断，重置读取位置")
        except Exception as e:
            logging.error(f"检查日志轮转失败: {e}")
            self._init_file()

    def run(self):
        """日志监控主逻辑"""
        pos_save_timer = time.time()
        while self.running:
            try:
                if not self.file_handle:
                    time.sleep(1)
                    self._init_file()
                    continue

                # 检查日志轮转
                self._check_log_rotate()

                # 读取新增内容
                new_content = self.file_handle.read()
                if new_content:
                    lines = new_content.splitlines()
                    self.file_pos = self.file_handle.tell()
                    # 解析日志行
                    for line in lines:
                        self.conn_state.update_accept_conn(line)
                        self.conn_state.update_remote_conn(line)
                        self.conn_state.update_close_conn(line)

                # 定期保存日志位置
                if time.time() - pos_save_timer >= LOG_POS_INTERVAL:
                    self._save_log_pos()
                    pos_save_timer = time.time()

                time.sleep(LOG_MONITOR_INTERVAL)
            except Exception as e:
                logging.error(f"日志监控异常: {e}")
                time.sleep(1)

    def stop(self):
        """停止监控"""
        self.running = False
        if self.file_handle:
            self.file_handle.close()
        self._save_log_pos()  # 最后一次保存位置

# ======================== HTTP接口/持久化线程（无修改） ========================
class ConnMonitorHTTPHandler(http.server.BaseHTTPRequestHandler):
    conn_state: ConnectionState = None

    def _send_json_response(self, data: dict, status_code: int = 200):
        self.send_response(status_code)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8"))

    def do_GET(self):
        try:
            if self.path == "/api/online":
                online = self.conn_state.get_online_conns()
                self._send_json_response({"code":0, "msg":"success", "data":online})
            elif self.path == "/api/history":
                limit = int(self.headers.get("Limit", 1000))
                history = self.conn_state.get_history_conns(limit)
                self._send_json_response({"code":0, "msg":"success", "data":history})
            elif self.path == "/api/stats":
                stats = self.conn_state.get_stats()
                self._send_json_response({"code":0, "msg":"success", "data":stats})
            elif self.path == "/api/health":
                self._send_json_response({"code":0, "msg":"healthy", "data":{"timestamp":time.time()}})
            else:
                self._send_json_response({"code":404, "msg":"not found"}, 404)
        except Exception as e:
            logging.error(f"HTTP接口异常: {e}")
            self._send_json_response({"code":500, "msg":str(e)}, 500)

class PersistThread(threading.Thread):
    def __init__(self, conn_state: ConnectionState, interval: int = PERSIST_INTERVAL):
        super().__init__(daemon=True)
        self.conn_state = conn_state
        self.interval = interval
        self.running = True

    def run(self):
        while self.running:
            time.sleep(self.interval)
            self.conn_state._save_persist_data()
            self.conn_state.clean_timeout_conns()

    def stop(self):
        self.running = False

# ======================== 主程序 ========================
def main():
    # 配置日志
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler("/var/log/stunnel_monitor/monitor.log", encoding="utf-8")
        ]
    )

    # 初始化连接状态
    conn_state = ConnectionState()

    # 绑定HTTP接口
    ConnMonitorHTTPHandler.conn_state = conn_state

    # 启动日志监控（含重启回溯）
    log_monitor = StunnelLogMonitor(STUNNEL_LOG_PATH, conn_state)
    log_monitor.start()
    logging.info("日志监控线程启动（含重启回溯）")

    # 启动持久化线程
    persist_thread = PersistThread(conn_state)
    persist_thread.start()
    logging.info("持久化线程启动")

    # 启动HTTP服务
    server_address = (HTTP_HOST, HTTP_PORT)
    httpd = http.server.ThreadingHTTPServer(server_address, ConnMonitorHTTPHandler)
    logging.info(f"HTTP服务启动: http://{HTTP_HOST}:{HTTP_PORT}")

    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        logging.info("接收到停止信号，正在退出...")
        # 停止线程
        log_monitor.stop()
        persist_thread.stop()
        # 关闭HTTP服务
        httpd.shutdown()
        # 最后一次持久化
        conn_state._save_persist_data()
        logging.info("程序正常退出")

if __name__ == "__main__":
    main()



#!/usr/bin/env python3
import re
import sys
import os
import time
import threading
import argparse
from datetime import datetime
from collections import defaultdict

# ===================== 核心配置（根据实际环境调整）=====================
# 1. stunnel 日志根目录（所有 stunnel-port.log 存放路径）
STUNNEL_LOG_DIR = "/var/log/stunnel/"
# 2. 日志文件名匹配规则（匹配 stunnel-端口.log，如 stunnel-443.log）
STUNNEL_LOG_PATTERN = r"stunnel-(\d+)\.log"
# 3. stunnel 日志时间格式（需与 stunnel.conf 输出格式一致）
STUNNEL_TIME_FORMAT = "%Y.%m.%d %H:%M:%S"
# 4. 时区配置（若 stunnel 日志为本地时区，需对应修改）
LOCAL_TIMEZONE = "Asia/Shanghai"  # 替换为 VM 本地时区（如 UTC）

# ===================== 全局变量（线程安全）=====================
# 在线连接存储：key=(stunnel_port, client_ip, client_port) → value=连接信息字典
online_connections = dict()
# 线程锁：保护 online_connections 读写（多线程安全）
conn_lock = threading.Lock()

# ===================== 日志解析正则（仅解析 stunnel 原生日志）=====================
# 1. 连接建立日志行：2024.05.24 10:00:00 LOG5[1]: Service [xxx] accepted connection from 10.128.0.20:56789
CONNECT_PATTERN = re.compile(
    r'^(?P<log_time>\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: Service .+ accepted connection from (?P<client_ip>\d+\.\d+\.\d+\.\d+):(?P<client_port>\d+)'
)
# 2. 连接关闭日志行：2024.05.24 10:01:00 LOG5[1]: Connection closed: 10.128.0.20:56789 → 10.0.0.10:8080
CLOSE_PATTERN = re.compile(
    r'^(?P<log_time>\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}) LOG\d+\[\d+\]: Connection closed: (?P<client_ip>\d+\.\d+\.\d+\.\d+):(?P<client_port>\d+) → .+'
)

# ===================== 单日志文件实时监听（tail -f）=====================
def tail_single_log(log_path: str, stunnel_port: str):
    """
    实时监听单个 stunnel 日志文件
    :param log_path: 日志文件路径（如 /var/log/stunnel/stunnel-443.log）
    :param stunnel_port: stunnel 监听端口（从文件名提取，如 443）
    """
    print(f"[INFO] 开始监听日志：{log_path}（对应 stunnel 端口：{stunnel_port}）")
    try:
        with open(log_path, "rb") as f:
            # 移动到文件末尾，只读取新内容
            f.seek(0, os.SEEK_END)
            # 记录文件 inode（用于检测日志轮转）
            file_inode = os.fstat(f.fileno()).st_ino

            while True:
                # 检测日志轮转（文件 inode 变化，如 logrotate 重命名后重建）
                try:
                    new_inode = os.stat(log_path).st_ino
                    if new_inode != file_inode:
                        print(f"[INFO] 检测到日志轮转：{log_path}，重新打开文件")
                        f.close()
                        f = open(log_path, "rb")
                        f.seek(0, os.SEEK_END)
                        file_inode = new_inode
                except FileNotFoundError:
                    # 日志文件被删除（轮转中），等待 1 秒重试
                    time.sleep(1)
                    continue

                # 读取新行（无内容则等待 0.1 秒）
                line = f.readline()
                if not line:
                    time.sleep(0.1)
                    continue

                # 解码日志行（忽略编码错误）
                line_str = line.decode("utf-8", errors="ignore").strip()
                # 解析日志行并更新在线连接
                parse_log_line(line_str, stunnel_port)

    except Exception as e:
        print(f"[ERROR] 监听日志 {log_path} 失败：{str(e)}")

# ===================== 日志行解析（连接建立/关闭）=====================
def parse_log_line(line_str: str, stunnel_port: str):
    """
    解析单条 stunnel 日志行，更新在线连接状态
    :param line_str: 日志行字符串
    :param stunnel_port: stunnel 监听端口
    """
    # 1. 解析「连接建立」日志
    connect_match = CONNECT_PATTERN.match(line_str)
    if connect_match:
        try:
            # 提取日志字段
            client_ip = connect_match.group("client_ip")
            client_port = connect_match.group("client_port")
            log_time_str = connect_match.group("log_time")
            # 转换日志时间为 datetime 对象
            log_time = datetime.strptime(log_time_str, STUNNEL_TIME_FORMAT)

            # 构建连接唯一标识（避免不同 stunnel 服务的同 IP:Port 混淆）
            conn_key = (stunnel_port, client_ip, client_port)
            # 加锁更新在线连接（线程安全）
            with conn_lock:
                online_connections[conn_key] = {
                    "stunnel_port": stunnel_port,
                    "client_ip": client_ip,
                    "client_port": client_port,
                    "connect_time": log_time,
                    "online_duration": 0  # 在线时长（秒），实时计算
                }

            print(f"[CONNECT] stunnel:{stunnel_port} | 客户端 {client_ip}:{client_port} 建立连接 | 时间：{log_time_str}")
        except Exception as e:
            print(f"[ERROR] 解析连接建立日志失败：{line_str} → {str(e)}")
        return

    # 2. 解析「连接关闭」日志
    close_match = CLOSE_PATTERN.match(line_str)
    if close_match:
        try:
            # 提取日志字段
            client_ip = close_match.group("client_ip")
            client_port = close_match.group("client_port")
            log_time_str = close_match.group("log_time")
            log_time = datetime.strptime(log_time_str, STUNNEL_TIME_FORMAT)

            # 构建连接唯一标识，从在线连接中移除
            conn_key = (stunnel_port, client_ip, client_port)
            with conn_lock:
                if conn_key in online_connections:
                    # 计算连接时长
                    conn_info = online_connections.pop(conn_key)
                    connect_time = conn_info["connect_time"]
                    duration = (log_time - connect_time).total_seconds()
                    print(f"[CLOSE] stunnel:{stunnel_port} | 客户端 {client_ip}:{client_port} 断开连接 | 时间：{log_time_str} | 连接时长：{duration:.2f} 秒")
                else:
                    print(f"[WARN] stunnel:{stunnel_port} | 客户端 {client_ip}:{client_port} 关闭连接，但未找到建立记录")
        except Exception as e:
            print(f"[ERROR] 解析连接关闭日志失败：{line_str} → {str(e)}")
        return

# ===================== 扫描所有 stunnel 日志并启动监听线程 =====================
def scan_and_monitor_all_logs():
    """扫描所有 stunnel-*.log 文件，启动独立线程监听"""
    print(f"[INFO] 开始扫描日志目录：{STUNNEL_LOG_DIR}")
    monitor_threads = []

    # 遍历日志目录，匹配 stunnel-端口.log 格式
    for filename in os.listdir(STUNNEL_LOG_DIR):
        log_match = re.match(STUNNEL_LOG_PATTERN, filename)
        if log_match:
            # 从文件名提取 stunnel 端口（如 stunnel-443.log → 443）
            stunnel_port = log_match.group(1)
            log_path = os.path.join(STUNNEL_LOG_DIR, filename)
            # 启动独立线程监听单个日志文件（守护线程，随主线程退出）
            t = threading.Thread(
                target=tail_single_log,
                args=(log_path, stunnel_port),
                daemon=True
            )
            t.start()
            monitor_threads.append(t)
            time.sleep(0.1)  # 避免同时启动过多线程

    # 无匹配的日志文件
    if not monitor_threads:
        print(f"[WARN] 未找到匹配的日志文件（{STUNNEL_LOG_PATTERN}），请检查日志路径和命名")
        sys.exit(1)

    print(f"[INFO] 共启动 {len(monitor_threads)} 个日志监听线程")
    # 等待所有线程（主线程阻塞，不退出）
    for t in monitor_threads:
        t.join()

# ===================== 打印所有在线连接 =====================
def print_online_connections():
    """控制台打印所有 stunnel 服务的在线连接"""
    print("\n" + "="*80)
    print(f"所有 stunnel 服务在线连接汇总 | 统计时间：{datetime.now().strftime(STUNNEL_TIME_FORMAT)}")
    print("="*80)

    with conn_lock:
        if not online_connections:
            print("无在线连接")
        else:
            # 按 stunnel 端口分组展示
            grouped_conns = defaultdict(list)
            for conn_key, conn_info in online_connections.items():
                grouped_conns[conn_info["stunnel_port"]].append(conn_info)

            # 遍历每个 stunnel 端口的在线连接
            for st_port, conn_list in grouped_conns.items():
                print(f"\nstunnel 端口 {st_port} | 在线连接数：{len(conn_list)}")
                print("-"*60)
                for conn in conn_list:
                    # 实时计算在线时长
                    online_duration = (datetime.now() - conn["connect_time"]).total_seconds()
                    print(
                        f"客户端 IP: {conn['client_ip']}:{conn['client_port']} | "
                        f"建立时间: {conn['connect_time'].strftime(STUNNEL_TIME_FORMAT)} | "
                        f"在线时长: {online_duration:.2f} 秒"
                    )

    print("="*80 + "\n")

# ===================== 主函数（命令行参数处理）=====================
def main():
    # 解析命令行参数
    parser = argparse.ArgumentParser(description="Stunnel 多日志监听 & 在线连接监控工具（无 GCP 依赖）")
    parser.add_argument(
        "--online",
        action="store_true",
        help="仅打印当前所有在线连接并退出（不启动日志监听）"
    )
    args = parser.parse_args()

    # 仅查询在线连接
    if args.online:
        print_online_connections()
        sys.exit(0)

    # 启动日志监听（主线程阻塞）
    try:
        scan_and_monitor_all_logs()
    except KeyboardInterrupt:
        print("\n[INFO] 手动终止监听，打印最终在线连接：")
        print_online_connections()
        sys.exit(0)
    except Exception as e:
        print(f"[FATAL] 启动监听失败：{str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()